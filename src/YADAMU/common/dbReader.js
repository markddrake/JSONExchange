"use strict";

const { performance } = require('perf_hooks');
const util = require('util')
const Readable = require('stream').Readable;
const stream = require('stream')
const PassThrough = stream.PassThrough
const pipeline = util.promisify(stream.pipeline);
const streamFinished = stream.finished

const Yadamu = require('./yadamu.js')
const YadamuLibrary = require('./yadamuLibrary.js')
const YadamuWriter = require('./yadamuWriter.js')
const Pushable = require('./pushable.js')
const {YadamuError, DatabaseError, IterativeInsertError, InputStreamError} = require('./yadamuException.js')

const YadamuConstants = require('./yadamuConstants.js')

class TableSwitcher extends PassThrough {

  constructor(tableName) {
	super()
	this.tableName = tableName
  }
  
  pipe(os,options) {
    options = options || {}
	options.end = false;
	return super.pipe(os,options)
  }
  
}

class DBReader extends Readable {  

  /* 
  **
  ** The DBReader is responsibe for replicating the source data source to the target source.
  **
  ** The DBReader starts by sending "systemInformation", "ddl" "metadata" and "pause" messages directly to the DBWriter.
  ** Then it waits for the DBWriter to raise 'ddlCompelete' before sending the contents of the tables.
  **
  ** A seperate set of table level readers and writers are used to send the table contents.
  ** If the target is a database, the table level operations can execute sequentially or in parallel.
  ** If the target is a file the table level operations must operate sequentially.
  ** 
  ** When the DBWriter recieves a pause message it caches the associated callback instead of invoking it. This has the effect of pausing the DBWriter. 
  ** When the DBReader has finished processing all the tables it resumes the DBWriter by invoking the cached callback.
  **
  ** #### The following comments are outdated and need to be updated once regressions have been run successfully ###
  **
  ** Once the 'Metadata' event has been sent the reader pauses itself. When the DBWriter has executed all of the DDL statements
  ** it emits a 'ddlComplete' event indicating that the target envrironment is ready to consume data. At this point the DBReader starts sending data. 
  ** 
  ** For database backed data sources, the DBReader does not send the data iteself. It creates reader/writer pairs for each table to be processed. There can be executed serially, one table at a time, 
  ** on in parallel allowing multiple tables to be processed simultaneously. Serial readers and writers share a database connection with the DBReader and DBWriter. Parallel readers and writers are allocated a
  ** dedicated database connection. 
  **
  ** When processing a file, which, by its very nature is serial, the DBReader generates the data as well as the metadata. When the DBReader recieves the ddlComplete notifciation switches the DBReader pipe 
  ** switches the pipe to 
  **
  ** When reading data from a file the JSON Parser (TextParser or HTMLParser) is the event source. The parser sends the data directly to the DBWriter, bypassing the DBReader. 
  ** The 'systemInformation', 'ddl' and 'metadata' events are processed directly by the DBWriter. Since parallelism is not possible with a sequential event source, 
  ** such as a file or HTML stream, a single worker processes all the data sent by the parser. The event stream automatically switches from the DBWriter to the Worker
  ** when the DBWriter emits the 'ddlComplete' event.
  **
  ** When the parser reaches the end of the file it emits an 'eof' event. When the worker recieves the 'eof' event it invokes it's end() method. This causes the _final()
  ** method to be to be executed  and a 'workerComplete' message is sent to the DBWriter before the worker shuts down. 
  **
  ** Once the DBWriter has received 'workerComplete' messages from all workers, it emits a 'dataComplete' event.
  **
  ** When the DBReader receives the 'dataComplete' notification it finalizes the export, releasing all resources. 
  **
  ** For a parallel event source, such as a database, where the DBReader controls the event flow, the DBReaders pushes an 'exportComplete' message into the pipe and then
  ** sets nextPhase to 'done' so that the DBReader is destroyed on the next '_read' event.
  **
  ** For a sequential event source, the DBReader is not the event source so the 'exportComplete' message is generated by invoking the exportComplete() method on the underlying DBI.
  ** 
  ** When the DBWriter receives the 'exportComplete' message it releases all of it's resources and invokes it's end() method. This causes the DBWriters '_final()' method 
  ** to be executed and a 'close' event to be emitted by the DBWriter. 
  ** 
  ** When the Yadamu data pump receives notification of the 'close' event it resolves the promise that wraps the DBReader -> DBWriter pipe operation.
  **
  */

  constructor(dbi,yadamuLogger,options) {

    super({objectMode: true });  
 
    this.dbi = dbi;
    this.status = dbi.yadamu.STATUS
    this.yadamuLogger = yadamuLogger;
    this.yadamuLogger.info([`Reader`,dbi.DATABASE_VENDOR,dbi.DB_VERSION,this.dbi.MODE,this.dbi.getWorkerNumber()],`Ready.`)
  
    this.metadata = undefined
    this.schemaInfo = [];
  
    this.nextPhase = 'systemInformation'
    this.dbWriter = undefined;
	
	this.dataComplete = undefined;
    this.copyStarted = false
    this.copyComplete = false
	
  }

  copyInProgress() {
	return this.copyStarted && !this.copyComplete
  }

  isDatabase() {
    return this.dbi.isDatabase()
  }
    
  
  pipe(outputStream,options) {
	this.dbWriter = outputStream
	return super.pipe(outputStream,options);
  } 
  
  async initialize() {
	await this.dbi.initializeExport() 
  }
  
  async getSystemInformation(version) {
    return this.dbi.getSystemInformation(version)
  }
  
  async getDDLOperations() {
	const startTime = performance.now();
    const ddl = await this.dbi.getDDLOperations()
	if (ddl !== undefined) {
      this.yadamuLogger.ddl([this.dbi.DATABASE_VENDOR],`Generated ${ddl.length} DDL statements. Elapsed time: ${YadamuLibrary.stringifyDuration(performance.now() - startTime)}s.`);
	}
	return ddl
  }
  
  async getMetadata() {
      
     const startTime = performance.now();
     this.schemaInfo = await this.dbi.getSchemaInfo('FROM_USER')
     this.yadamuLogger.ddl([this.dbi.DATABASE_VENDOR],`Generated metadata for ${this.schemaInfo.length} tables. Elapsed time: ${YadamuLibrary.stringifyDuration(performance.now() - startTime)}s.`);
     return this.dbi.generateMetadata(this.schemaInfo)
  }
  
  traceSteamEvents(streams) {

    // Add event tracing to the streams
	  
    streams.forEach((s,idx) => {
	  s.once('end',() => {
	     console.log(s.constructor.name,'end')
	  }).once('finish', (err) => {
	    console.log(s.constructor.name,'finish')
	  }).once('close', (err) => {
        console.log(s.constructor.name,'close')
	  }).once('error', (err) => {
	    // console.log(s.constructor.name,'error',err)
	    console.log(s.constructor.name,'error',err.message)
	  })
	})
  }
  
  async pipelineTable(task,readerDBI,writerDBI) {
	 
    let tableInfo
	let mappedTableName
	let tableOutputStream

	const yadamuPipeline = []
	let streamEnded
	let streamFailed
   
    try {
      tableInfo = readerDBI.generateQueryInformation(task)
  
	  const inputStreams = await readerDBI.getInputStreams(tableInfo)
      yadamuPipeline.push(...inputStreams)
      mappedTableName = writerDBI.transformTableName(task.TABLE_NAME,readerDBI.getInverseTableMappings())
	  const outputStreams = await writerDBI.getOutputStreams(mappedTableName,this.dbWriter.ddlComplete)
	  yadamuPipeline.push(...outputStreams)

      tableOutputStream = outputStreams[0]
      tableOutputStream.setReaderMetrics(readerDBI.INPUT_METRICS)
	  
	  const terminalStream = yadamuPipeline[yadamuPipeline.length-1]
	  if (terminalStream instanceof stream.PassThrough) {
		// Pipelines that terminate in a PassThrough stream seem not to destroy all of the components correctly when the data has been processed. 
		// This causes the operation to hang. Add a listener to the PassThrough's close event to destroy all of the componants of the pipeline
	    // this.yadamuLogger.trace([this.constructor.name,'PIPELINE',mappedTableName,readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],`Pipleine terminates in a PassThrough stream`)
    	terminalStream.once('close',() => {
          yadamuPipeline.map((proc) => {if (!proc.destroyed) {proc.destroy()}})
	    })
	  }
    } catch (e) {
      this.yadamuLogger.handleException(['PIPELINE','STREAM INITIALIZATION',task.TABLE_NAME,readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],e)
      throw (e)
    }

    // addEventStreamTracing(yadamuPipeline)
	
	// Promise that resovles when the writer has completed all processing...
	const writerComplete = new Promise((resolve,reject) => {
	  tableOutputStream.on('writerComplete',(err) => {
	    resolve(err)
	  })
    })
	
	try {
	  // this.yadamuLogger.trace([this.constructor.name,'PIPELINE',mappedTableName,readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],`${yadamuPipeline.map((proc) => { return proc.constructor.name }).join(' => ')}`)
      readerDBI.INPUT_METRICS.pipeStartTime = performance.now();
	  this.activePipelines.add(yadamuPipeline[0])
	  await pipeline(yadamuPipeline)
	  this.activePipelines.delete(yadamuPipeline[0])  
	  readerDBI.INPUT_METRICS.pipeEndTime = performance.now();
	  const cause = await writerComplete
	  if (cause instanceof Error) {
	    throw cause
	  }
      //this.yadamuLogger.trace([this.constructor.name,'PIPELINE',mappedTableName,readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR,'SUCCESS'],writerComplete)
	} catch (err) {
	  // this.yadamuLogger.trace([this.constructor.name,'PIPELINE',mappedTableName,readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR,readerDBI.yadamu.ON_ERROR,'FAILED'],`${err.constructor.name},${err.message}`)
	  this.activePipelines.delete(yadamuPipeline[0])
	        
	  // Wait for DDL operations to complete. If ddlComplete operations fail no data will be written to the target database. ddlComplete will throws an exception which will be caught by pipelineTables(). 
	  await this.dbWriter.ddlComplete
	  
	  // When an error occurs not all rows read have been written to the target database. Wait for the writer to indicate that is has finished processing outstanding rows.
      const cause = await writerComplete
	  
	  // this.yadamuLogger.trace([this.constructor.name,'PIPELINE',mappedTableName,readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],`${yadamuPipeline.map((proc) => { return `${proc.constructor.name}:${proc.destroyed}` }).join(' => ')}`)
      // yadamuPipeline.forEach((s,i) => { console.log(i,s.constructor.name); if (s.dbi ) console.log(s.dbi.firstError, s.dbi.latestError)})
	  
	  // Throw the error if ON_ERROR handling is ABORT
      if (readerDBI.yadamu.ON_ERROR === 'ABORT') {
  	    throw cause;
      }
      
	  if (YadamuError.closedConnection(readerDBI.INPUT_METRICS.readerError)) {
        // Re-establish the input stream connection 
   	    await readerDBI.reconnect(cause,'READER')
      }
      this.dbi.resetExceptionTracking()
    }
  }
  
  async pipelineTables(readerDBI,writerDBI) {
	 
	this.activePipelines = new Set();
	  
	let tableCount = this.schemaInfo.filter((task) => { return task.INCLUDE_TABLE }).length
	if (this.schemaInfo.length > 0) {
      this.yadamuLogger.info(['PIPELINE','SEQUENTIAL',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],`Processing ${tableCount} Tables`);
	  for (const task of this.schemaInfo) {
        if (task.INCLUDE_TABLE === true) {
	      try {
			tableCount--
            await this.pipelineTable(task,readerDBI,writerDBI)
	      } catch (cause) {
	        // this.yadamuLogger.trace(['PIPELINE','SEQUENTIAL',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],cause)
			this.yadamuLogger.handleException(['PIPELINE','SEQUENTIAL',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR,readerDBI.ON_ERROR],cause)
			if (tableCount > 0) {
		      this.yadamuLogger.error(['PIPELINE','SEQUENTIAL',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR,readerDBI.ON_ERROR],`Operation failed: Skipping ${tableCount} Tables`);
			}
			else {
		      this.yadamuLogger.warning(['PIPELINE','SEQUENTIAL',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR,readerDBI.ON_ERROR],`Operation failed.`);
			}				
	        // Throwing here raises 'ERR_STREAM_PREMATURE_CLOSE' on the Writer. Cache the cause 
			this.underlyingError = cause;
            throw cause
          }
	    }
	  }
    }
  }
  
  async pipelineTableToFile(readerDBI,writerDBI,taskList,idx) {
	 
	 const task = taskList.shift();
	 
	 const tableInfo = readerDBI.generateQueryInformation(task)
	 
	 // Get the Table Readers 
	 const sourcePipeline = await readerDBI.getInputStreams(tableInfo)
	 const mappedTableName = writerDBI.transformTableName(task.TABLE_NAME,readerDBI.getInverseTableMappings())
	 
	 // Create a JSON Writer
	 const jsonWriter = await writerDBI.getOutputStream(mappedTableName,undefined,idx)
     jsonWriter.setReaderMetrics(readerDBI.INPUT_METRICS)
	 sourcePipeline.push(jsonWriter)
	 
	 // The TableManager is used to prevent 'end' events propegating to the output stream
	 const tableSwitcher = new TableSwitcher(mappedTableName) 
	 sourcePipeline.push(tableSwitcher)
    
     const targetPipeline = [...writerDBI.getOutputStreams(mappedTableName)]
	 const yadamuPipeline = new Array(...sourcePipeline,...targetPipeline)
   	 
	 // When the JSON Writer finishes emit an event which will create the next pipeline
	 streamFinished(tableSwitcher,() => {
	   // Manually clean up the previous pipeline since it never completely ended. Prevents excessive memory usage..
	   // Remove unpipe listeners on targets
	   targetPipeline.forEach((s) => { s.removeAllListeners('unpipe') })
	   // Unpipe the tableSwitcher`
	   
	   tableSwitcher.unpipe(writerDBI.PIPELINE_ENTRY_POINT)
	   
	   // Unpipe all target streams
	   targetPipeline.forEach((s,i) => { if (i < targetPipeline.length - 1) {s.unpipe(targetPipeline[i+1])} })
	   
	   // Destroy the source streams
	   sourcePipeline.forEach((s) => { s.destroy() })

	   if  (taskList.length > 0) {
		 this.emit('nextTable',idx+1)
	   }
       else {
		 this.emit('dataComplete')
       }	
	 })

     // addEventStreamTracing(yadamuPipeline)
	  
     // this.yadamuLogger.trace([this.constructor.name,'PIPELINE',tableInfo.TABLE_NAME,readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],`${yadamuPipeline.map((proc) => { return proc.constructor.name }).join(' => ')}`)
	 pipeline(yadamuPipeline)

  }

  async pipelineTablesToFile(readerDBI,writerDBI) {
	
  	  await this.dbWriter.ddlComplete
	  const taskList = this.schemaInfo.filter((task) => { return task.INCLUDE_TABLE })
      this.yadamuLogger.info(['PIPELINE','SERIAL',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],`Processing ${taskList.length} Tables`);

	  this.on('nextTable',async (idx) => {
		 await this.pipelineTableToFile(readerDBI,writerDBI,taskList,idx)
	  })

	  this.emit('nextTable',0);
	  
	  await new Promise((resolve,reject) => {
		this.on('dataComplete',() => {
		  resolve()
		})
	  })  
     
	  await pipeline(writerDBI.END_EXPORT_FILE,writerDBI.PIPELINE_ENTRY_POINT)
	  
  }

  async generateStatementCache(metadata) {
    if (!YadamuLibrary.isEmpty(metadata)) {   
      // ### if the import already processed a DDL object do not execute DDL when generating statements.
      Object.keys(metadata).forEach((table) => {
         metadata[table].vendor = this.dbi.systemInformation.vendor;
      })
    }
    this.dbi.setMetadata(metadata)      
    await this.dbi.generateStatementCache('%%SCHEMA%%',false)
  }
  
  getInputStreams() {
	  
	/*
	**
	** For a database the DBReader class is repsonsible for generating the events in the required order. This is handled via the 'nextPhase' setting in the _read method.
	** Thus the DBReader becomes the event stream.
	**
	** For a File the events are generated according to the contents of the file.
	** The file parser becomes the event stream.
	*/
	
    if (this.dbi.isDatabase()){
      return [this]
    }
    else {
	  return this.dbi.getInputStreams()
    }

  }
 
  async _read() {
    try {
 	  // this.yadamuLogger.trace([this.constructor.name,`_READ()`,this.dbi.DATABASE_VENDOR],this.nextPhase)
      switch (this.nextPhase) {
         case 'systemInformation' :
           const systemInformation = await this.getSystemInformation();
		   // Needed in case we have to generate DDL from the system information and metadata.
           this.dbi.setSystemInformation(systemInformation);
		   this.dbi.yadamu.REJECTION_MANAGER.setSystemInformation(systemInformation)
		   this.dbi.yadamu.WARNING_MANAGER.setSystemInformation(systemInformation)
           this.push({systemInformation : systemInformation});
           if (this.dbi.MODE === 'DATA_ONLY') {
             this.nextPhase = 'metadata';
           }
           else { 
             this.nextPhase = 'ddl';
           }
           break;
         case 'ddl' :
           let ddl = await this.getDDLOperations();
           if (ddl === undefined) {
             // Database does not provide mechansim to retrieve DDL statements used to create a schema (eg Oracle's DBMS_METADATA package)
             // Reverse Engineer DDL from metadata.
             this.metadata = await this.getMetadata();
             await this.generateStatementCache(this.metadata)
             ddl = Object.values(this.dbi.statementCache).map((table) => {
               return table.ddl
             })
           } 
           this.push({ddl: ddl});
		   this.nextPhase = 'metadata';
           break;
         case 'metadata' :
           this.metadata = this.metadata ? this.metadata : await this.getMetadata();
           this.push({metadata: this.dbi.transformMetadata(this.metadata,this.dbi.inverseTableMappings)});
		   this.dbi.yadamu.REJECTION_MANAGER.setMetadata(this.metadata)
		   this.dbi.yadamu.WARNING_MANAGER.setMetadata(this.metadata)
		   this.nextPhase = ((this.dbi.MODE === 'DDL_ONLY') || (this.schemaInfo.length === 0)) ? 'exportComplete' : 'copyData';
		   break;
		 case 'copyData':
   	 	   this.copyStarted = true;
           this.dataComplete = new Promise((resolve,reject) => {
             this.once('dataComplete',(status) => {
               // this.yadamuLogger.trace([this.constructor.name],`${this.constructor.name}.on(dataComplete): (${status instanceof Error}) "${status ? `${status.constructor.name}(${status.message})` : status}"`)
               this.copyComplete = true
           	   if (status instanceof Error) reject(status)
           	   resolve(true);
             })
           })	
           if (this.dbWriter.dbi.isDatabase()) {
		     await this.pipelineTables(this.dbi,this.dbWriter.dbi);
	       }
	       else {
	         await this.pipelineTablesToFile(this.dbi,this.dbWriter.dbi);
           }
		   this.emit('dataComplete')
		   // No 'break' - fall through to 'exportComplete'.
		 case 'exportComplete':
	       // this.yadamuLogger.trace([this.constructor.name,'DLL_COMPLETE',this.dbi.getWorkerNumber()],'WAITING')
	       await this.dbWriter.ddlComplete
           // this.yadamuLogger.trace([this.constructor.name,'DLL_COMPLETE',this.dbi.getWorkerNumber()],'PROCESSING')
		   this.dbWriter.callDeferredCallback()
		   this.push(null);
		   break;
	    default:
      }
    } catch (cause) {
	  // Don't pass the error to dataComplete or we get unhandled exceptions from the throw...
  	  this.emit('dataComplete');
	  this.yadamuLogger.handleException([`READER`,this.dbi.DATABASE_VENDOR,`_READ(${this.nextPhase})`,this.dbi.yadamu.ON_ERROR],cause);
	  this.underlyingError = cause;
	  await this.dbi.releasePrimaryConnection();
      this.destroy(cause)
    }
  }  
   
  async _destroy(cause,callback) {
    // this.yadamuLogger.trace([this.constructor.name,this.dbi.isDatabase()],'_destroy()')
    try {
	  await this.dbi.finalizeExport();
	  await this.dbi.releasePrimaryConnection();
	  callback()
	} catch (e) {
      if (YadamuError.closedConnection(cause)) {
        callback(cause)
	  }
	  else {
        this.yadamuLogger.handleException([`READER`,this.dbi.DATABASE_VENDOR,`_DESTROY()`,this.dbi.yadamu.ON_ERROR],e);		
        callback(e)
      }
    }
  }
  
}

module.exports = DBReader;