"use strict";

const { performance } = require('perf_hooks');

const Yadamu = require('../common/yadamu.js')
const YadamuLibrary = require('../common/yadamuLibrary.js')
const DBReader = require('../common/dbReader.js');									 

class DBReaderParallel extends DBReader {  

  constructor(dbi,mode,status,yadamuLogger,options) {
    super(dbi,mode,status,yadamuLogger,options); 
    this.dbi.sqlTraceTag = `/* Primary */`;	
  }
    
  async processTables() {
	  
	/*
	**
	** Create a Pool of Yadamu Writers that will execute operations table copy operations.
	** The number of readers and writers is determined by the parameter PARALLEL
	** Each Reader and Writer is allocated it's own connection from the master's connection pool.
	** The same connection will be used to read all the tables proccessed.
	** Each Reader/Writer pair is assinged a table to process. 
	** Once the copy is complete the Reader / Writer pair is assigned a new table to process.
	** When all tables have been processed the Database Connections are released.
	**
	*/
	
	if (this.schemaInfo.length > 0) {
  	  const maxWorkerCount = parseInt(this.dbi.parameters.PARALLEL)
  	  const workerCount = this.schemaInfo.length < maxWorkerCount ? this.schemaInfo.length : maxWorkerCount
	  this.yadamuLogger.info(['PARALLEL',this.dbi.DATABASE_VENDOR],`Worker Count: ${workerCount}.`);
	  this.writer.initializeWorkers(workerCount);
      const tasks = [...this.schemaInfo]
      const parallelTasks = Array(workerCount).fill(0)
      parallelTasks.forEach(async (dummy,idx) => {
        const workerReaderDBI = await this.dbi.workerDBI(idx);
	    const workerDBI = await this.writer.dbi.workerDBI(idx)		 
        const worker = workerDBI.getOutputStream(this.writer);
	    while (tasks.length > 0) {
		  const task = tasks.shift();
          // The copyOperation is a promise that wraps a pipe operation that copy rows from the reader via a parser to the writer
		  // The opeartion resolves when the parser has sent all the rows generated by the reader to the writer.
		  // copyOperation does not wait for the writer to finish. This allows the next read operation to commence 
		  // while the writer is still processing rows from the previous table.
          const copyOperation = this.createCopyOperation(workerReaderDBI,task,worker)
          try {
            // this.yadamuLogger.trace([`${this.constructor.name}`,`${task.TABLE_NAME}`,`START`],``)
            const readerStats = await copyOperation
		    readerStats.tableName = task.TABLE_NAME
		    worker.write({eod:readerStats});
		    // this.yadamuLogger.trace([`${this.constructor.name}`,`${task.TABLE_NAME}`,`SUCCESS`],`Rows read: ${stats.rowsRead}. Elaspsed Time: ${YadamuLibrary.stringifyDuration(elapsedTime)}s. Throughput: ${Math.round((stats.rowsRead/elapsedTime) * 1000)} rows/s.`)
       	  } catch(e) {
  		    // Worker raised unrecoverable error. Worker cannot process any more tables.
            // this.yadamuLogger.trace([`${this.constructor.name}`,`${task.TABLE_NAME}`,`FAILED`],`Rows read: ${stats.rowsRead}.`)
            this.yadamuLogger.handleException(['PARALLEL','Reader',workerReaderDBI.DATABASE_VENDOR,task.TABLE_NAME,`COPY`],e)
		    this.writer.setWorkerException(e);
		    if (['FLUSH','SKIP'].indexOf(this.dbi.parameters.ON_ERROR) === -1) {
              tasks.length = 0;
			  // ### TODO - Signal abort() to active reader/writer pairings...
		    }
		    break;
          }
        }
	    await workerReaderDBI.releaseWorkerConnection()
	    // ### Need to wait for Writer to Finish before closing writer's connection 
        worker.end()
	  })
	
	  // All workers terminated and connections closed
	}
	else {
      this.yadamuLogger.info(['PARALLEL',this.dbi.DATABASE_VENDOR],`No tables found.`);
    }	
  }
      
}

module.exports = DBReaderParallel;