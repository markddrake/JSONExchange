"use strict";
const Readable = require('stream').Readable;
const Yadamu = require('./yadamu.js')
const YadamuLibrary = require('./yadamuLibrary.js')
const {DatabaseError} = require('./yadamuError.js')
const { performance } = require('perf_hooks');

const util = require('util')
const stream = require('stream')
const pipeline = util.promisify(stream.pipeline);

class DBReader extends Readable {  

  /* 
  **
  ** The DBReaderis responsibe for replicating the source data source to the target source.
  **
  ** The DBReader starts by sending "systemInformation", "ddl" "metadata" and "pause" messages directly to the DBWriter.
  ** Then it waits for the DBWriter to raise 'ddlCompelete' before sending the contents of the tables.
  **
  ** A seperate set of table level readers and writers are used to send the table contents.
  ** If the target is a database, the table level operations can execute sequentially or in parallel.
  ** If the target is a file the table level operations must operate sequentially.
  ** 
  ** When the DBWriter recieves a pause message it caches the associated callback instead of invoking it. This has the effect of pausing the DBWriter. 
  ** When the DBReader has finished processing all the tables it resumes the DBWriter by invoking the cached callback.
  **
  ** #### The following comments are outdated and need to be updated once regressions have been run successfully ###
  **
  ** Once the 'Metadata' event has been sent the reader pauses itself. When the DBWriter has executed all of the DDL statements
  ** it emits a 'ddlComplete' event indicating that the target envrironment is ready to consume data. At this point the DBReader starts sending data. 
  ** 
  ** For database backed data sources, the DBReader does not send the data iteself. It creates reader/writer pairs for each table to be processed. There can be executed serially, one table at a time, 
  ** on in parallel allowing multiple tables to be processed simultaneously. Serial readers and writers share a database connection with the DBReader and DBWriter. Parallel readers and writers are allocated a
  ** dedicated database connection. 
  **
  ** When processing a file, which, by its very nature is serial, the DBReader generates the data as well as the metadata. When the DBReader recieves the ddlComplete notifciation switches the DBReader pipe 
  ** switches the pipe to 
  **
  ** When reading data from a file the JSON Parser (TextParser or HTMLParser) is the event source. The parser sends the data directly to the DBWriter, bypassing the DBReader. 
  ** The 'systemInformation', 'ddl' and 'metadata' events are processed directly by the DBWriter. Since parallelism is not possible with a sequential event source, 
  ** such as a file or HTML stream, a single worker processes all the data sent by the parser. The event stream automatically switches from the DBWriter to the Worker
  ** when the DBWriter emits the 'ddlComplete' event.
  **
  ** When the parser reaches the end of the file it emits an 'eof' event. When the worker recieves the 'eof' event it invokes it's end() method. This causes the _final()
  ** method to be to be executed  and a 'workerComplete' message is sent to the DBWriter before the worker shuts down. 
  **
  ** Once the DBWriter has received 'workerComplete' messages from all workers, it emits a 'dataComplete' event.
  **
  ** When the DBReader receives the 'dataComplete' notification it finalizes the export, releasing all resources. 
  **
  ** For a parallel event source, such as a database, where the DBReader controls the event flow, the DBReaders pushes an 'exportComplete' message into the pipe and then
  ** sets nextPhase to 'done' so that the DBReader is destroyed on the next '_read' event.
  **
  ** For a sequential event source, the DBReader is not the event source so the 'exportComplete' message is generated by invoking the exportComplete() method on the underlying DBI.
  ** 
  ** When the DBWriter receives the 'exportComplete' message it releases all of it's resources and invokes it's end() method. This causes the DBWriters '_final()' method 
  ** to be executed and a 'close' event to be emitted by the DBWriter. 
  ** 
  ** When the Yadamu data pump receives notification of the 'close' event it resolves the promise that wraps the DBReader -> DBWriter pipe operation.
  **
  */

  constructor(dbi,yadamuLogger,options) {

    super({objectMode: true });  
 
    this.dbi = dbi;
    this.status = dbi.yadamu.STATUS
    this.yadamuLogger = yadamuLogger;
    this.yadamuLogger.info([`Reader`,dbi.DATABASE_VENDOR,this.dbi.MODE,this.dbi.getWorkerNumber()],`Ready.`)
       
    this.schemaInfo = [];
    
    this.nextPhase = 'systemInformation'
    this.ddlCompleted = false;
    this.dbWriter = undefined;
  }
 
  isDatabase() {
    return this.dbi.isDatabase()
  }
    
  
  pipe(outputStream,options) {
	this.dbWriter = outputStream
	this.ddlComplete = new Promise((resolve,reject) => {
	  this.dbWriter.once('ddlComplete',() => {
 	    // this.yadamuLogger.trace([this.constructor.name],`DDL Complete`)
		resolve(true);
	  })
	})
    return super.pipe(outputStream,options);
  } 
  
  async initialize() {
	await this.dbi.initializeExport() 
  }
  
  async getSystemInformation(version) {
    return this.dbi.getSystemInformation(version)
  }
  
  async getDDLOperations() {
	const startTime = performance.now();
    const ddl = await this.dbi.getDDLOperations()
	if (ddl !== undefined) {
      this.yadamuLogger.ddl([`${this.dbi.DATABASE_VENDOR}`],`Generated ${ddl.length} DDL statements. Elapsed time: ${YadamuLibrary.stringifyDuration(performance.now() - startTime)}s.`);
	}
	return ddl
  }
  
  async getMetadata() {
      
     const startTime = performance.now();
     this.schemaInfo = await this.dbi.getSchemaInfo('FROM_USER')
     this.yadamuLogger.ddl([`${this.dbi.DATABASE_VENDOR}`],`Generated metadata for ${this.schemaInfo.length} tables. Elapsed time: ${YadamuLibrary.stringifyDuration(performance.now() - startTime)}s.`);
     return this.dbi.generateMetadata(this.schemaInfo)
  }
     
  abortOnError(cause,dbi) {
	 const abortCodes = ['ABORT',undefined]
	 // dbi.setFatalError(cause);
	 return abortCodes.indexOf(dbi.yadamu.ON_ERROR) > -1
  }
  
  async pipelineTable(task,readerDBI,writerDBI) {
	 
    const abortCurrentTable = ['ABORT','SKIP']
    const continueProcessing = ['SKIP','FLUSH']
	
    const pipeStatistics = {
      rowsRead       : 0
    , pipeStartTime  : undefined
    , readerEndTime  : undefined
    , parserEndTime  : undefined
    , copyFailed     : false
    , tableNotFound  : false
    }
   
    let cause
	let stream
    let errorDBI
       
    try {
      const tableInfo = readerDBI.generateQueryInformation(task)
      // ### TESTING ONLY: Uncomment folllowing line to force Table Not Found condition
      // tableInfo.SQL_STATEMENT = tableInfo.SQL_STATEMENT.replace(tableInfo.TABLE_NAME,tableInfo.TABLE_NAME + "1")
      const transformer = readerDBI.createParser(tableInfo)
      const tableInputStream = await readerDBI.getInputStream(tableInfo,transformer)
      tableInputStream.on('error',(err) => { 
        pipeStatistics.readerEndTime = performance.now()
      }).on('end',() => {
        pipeStatistics.readerEndTime = performance.now()
      });
   
      const mappedTableName = writerDBI.transformTableName(task.TABLE_NAME,readerDBI.getInverseTableMappings())
      const tableOutputStream = writerDBI.getOutputStream(mappedTableName)
      transformer.on('error',(err) => { 
        pipeStatistics.parserEndTime = performance.now()
      })
    
   	  try {
        await tableOutputStream.initialize()
        pipeStatistics.pipeStartTime = performance.now();
        await pipeline(tableInputStream,transformer,tableOutputStream)
        // this.yadamuLogger.trace([this.constructor.name,'PIPELINE',mappedTableName,this.dbi.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],'SUCCESS')
      } catch (e) {
        cause = e
		stream = 'WRITER'
        errorDBI = writerDBI
        pipeStatistics.copyFailed = true
   
        if (!(e instanceof DatabaseError)) {
          // ### "Quack Quack" ### Need a better method to determine whether the reader or writer is responsible for the error.. 
          // When an error occurs pipeline activates the on error events for all of the streams in the pipline
          // Reader Errors are surfaced as raw database errors and need to be wrapped with the approiate Yadamu Error class.
          // Writer Errors are handled inside the Yadamu Writer instance and are wrapped in the approriate Yadamu Error class prior to being surfaced
          // this.yadamuLogger.warning([`${this.constructor.name}`,`${tableInfo.TABLE_NAME}`],`Reader failed at row: ${transformer.getCounter()+1}.`)
		  stream = 'READER'
    	  errorDBI = readerDBI
          cause = readerDBI.streamingError(e,tableInfo.SQL_STATEMENT)
          if ((continueProcessing.indexOf(readerDBI.yadamu.ON_ERROR) > -1)  && cause.lostConnection()) {
            // Re-establish the input stream connection 
   		    await readerDBI.reconnect(cause,'READER')
          }
        }		
        this.yadamuLogger.handleException(['PIPELINE',mappedTableName,this.dbi.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR,'STREAM PROCESSING',stream,errorDBI.yadamu.ON_ERROR],cause)
        if (abortCurrentTable.indexOf(writerDBI.yadamu.ON_ERROR) > -1) {
          tableOutputStream.abortWriter();
		}
        await tableOutputStream.forcedEnd(cause);
      }
      pipeStatistics.pipeEndTime = performance.now();
      pipeStatistics.rowsRead = transformer.getCounter()
      pipeStatistics.parserEndTime = transformer.endTime
      const timings = tableOutputStream.reportPerformance(pipeStatistics);
      this.dbWriter.recordTimings(timings);
   
      if (cause && (this.abortOnError(cause,errorDBI))) {
		throw cause;
      }
    } catch (e) {
      this.yadamuLogger.handleException(['PIPELINE',task.TABLE_NAME,this.dbi.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR,'STREAM CREATION'],e)
      throw (e)
    }
  }

  async pipelineTables(readerDBI,writerDBI) {
	if (this.schemaInfo.length > 0) {
      this.yadamuLogger.info(['SEQUENTIAL',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],`Processing Tables`);
	  for (const task of this.schemaInfo) {
        if (task.INCLUDE_TABLE === true) {
	      try {
            await this.pipelineTable(task,readerDBI,writerDBI)
	      } catch (cause) {
	        this.underlyingError = cause;
	        this.yadamuLogger.handleException(['SEQUENTIAL','PIPELINES',readerDBI.DATABASE_VENDOR,writerDBI.DATABASE_VENDOR],cause)
		    // Throwing here raises 'ERR_STREAM_PREMATURE_CLOSE' on the Writer. Cache the cause 
            throw(cause)
          }
	    }
	  }
    }
  }
  	
  async generateStatementCache(metadata) {
    if (Object.keys(metadata).length > 0) {   
      // ### if the import already processed a DDL object do not execute DDL when generating statements.
      Object.keys(metadata).forEach((table) => {
         metadata[table].vendor = this.dbi.systemInformation.vendor;
      })
    }
    this.dbi.setMetadata(metadata)      
    await this.dbi.generateStatementCache('%%SCHEMA%%',false)
  }
  
  getInputStream() {
	  
	/*
	**
	** For a database the DBReader class is repsonsible for generating the events in the required order. This is handled via the 'nextPhase' setting in the _read method.
	** Thus the DBReader becomes the event stream.
	**
	** For a File the events are generated according to the contents of the file.
	** The file parser becomes the event stream.
	*/
	
    if (this.dbi.isDatabase()){
      return this
    }
    else {
	  this.nextPhase = 'wait'
	  return this.dbi.getInputStream()
	  // return this
    }

  }
 
  async _read() {
    try {
 	  // this.yadamuLogger.trace([this.constructor.name,`_READ()`,this.dbi.DATABASE_VENDOR],this.nextPhase)
      switch (this.nextPhase) {
         case 'systemInformation' :
           const systemInformation = await this.getSystemInformation();
		   // Needed in case we have to generate DDL from the system information and metadata.
           this.dbi.setSystemInformation(systemInformation);
		   this.dbi.yadamu.REJECTION_MANAGER.setSystemInformation(systemInformation)
		   this.dbi.yadamu.WARNING_MANAGER.setSystemInformation(systemInformation)
           this.push({systemInformation : systemInformation});
           if (this.dbi.MODE === 'DATA_ONLY') {
             this.nextPhase = 'metadata';
           }
           else { 
             this.nextPhase = 'ddl';
           }
           break;
         case 'ddl' :
           let ddl = await this.getDDLOperations();
           if (ddl === undefined) {
             // Database does not provide mechansim to retrieve DDL statements used to create a schema (eg Oracle's DBMS_METADATA package)
             // Reverse Engineer DDL from metadata.
             const metadata = await this.getMetadata();
             await this.generateStatementCache(metadata)
             ddl = Object.keys(this.dbi.statementCache).map((table) => {
               return this.dbi.statementCache[table].ddl
             })
           } 
           this.push({ddl: ddl});
		   this.nextPhase = this.dbi.MODE === 'DDL_ONLY' ? 'exportComplete' : 'metadata';
           break;
         case 'metadata' :
           const metadata = await this.getMetadata();
           this.push({metadata: this.dbi.transformMetadata(metadata,this.dbi.inverseTableMappings)});
		   this.dbi.yadamu.REJECTION_MANAGER.setMetadata(metadata)
		   this.dbi.yadamu.WARNING_MANAGER.setMetadata(metadata)
		   this.nextPhase = 'pause';
		   break;
		 case 'pause':
	       this.push({pause:true})
		   this.nextPhase = 'copyData'
		   break;
		 case 'copyData':
		   await this.ddlComplete
		   await this.pipelineTables(this.dbi,this.dbWriter.dbi);
    	   // this.yadamuLogger.trace([this.constructor.name,,this.dbi.DATABASE_VENDOR,`_READ(${this.nextPhase})`,this.dbi.yadamu.ON_ERROR],'Exeucting Deferred Callback')
		   this.dbWriter.deferredCallback();
		   // No 'break' - fall through to 'exportComplete'.
		 case 'exportComplete':
		   this.push(null);
		   break;
	    default:
      }
    } catch (e) {
	  this.yadamuLogger.handleException([`READER`,this.dbi.DATABASE_VENDOR,`_READ(${this.nextPhase})`,this.dbi.yadamu.ON_ERROR],e);
	  this.underlyingError = e;
	  await this.dbi.releasePrimaryConnection();
      this.destroy(e)
    }
  }  
   
  async _destroy(cause,callback) {
    // this.yadamuLogger.trace([this.constructor.name,this.dbi.isDatabase()],'_destroy()')
    try {
      await this.dbi.finalizeExport();
	  await this.dbi.releasePrimaryConnection();
	  callback()
	} catch (e) {
      if (cause instanceof DatabaseError && cause.lostConnection()) {
        callback(cause)
	  }
	  else {
        this.yadamuLogger.handleException([`READER`,this.dbi.DATABASE_VENDOR,`_DESTROY()`,this.dbi.yadamu.ON_ERROR],e);		
        callback(e)
      }
    }
  }
  
}

module.exports = DBReader;