
"use strict" 
const fs = require('fs');
const util = require('util')
const Readable = require('stream').Readable;
const { performance } = require('perf_hooks');

/* 
**
** Require Database Vendors API 
**
*/

const MongoClient = require('mongodb').MongoClient

const Yadamu = require('../../common/yadamu.js');
const YadamuDBI =  require('../../common/yadamuDBI.js');
const MongoError = require('./mongoError.js')
const MongoWriter = require('./mongoWriter.js');
const MongoParser = require('./mongoParser.js');
const StatementGenerator = require('./statementGenerator.js');

/*
**
**  IMPORT : Implemented in TableWriter.js. 
**           MongoDB support allows data from a relational table to be imported using one of the following mappings
**
**  DOCUMENT: The source material must consist of a single JSON Object. 
**            The export format is still an array of arrays. The Array representing each row will have one member which must be valid JSON. e.g. "CollectionName" : [[JSON],[{JSON}]]
**            If the JSON is an object the object becomes the mongo document.
**            If the JSON is a scalar or an array then then a mongo document with a single key of "yadamuValue" will be inserted. The JSON will be the value assocoated with yadamuValue
**            ### ToDo : Allow name of key to be supplied by user
**  
**  OBJECT:   The source material consists of a one or more non-JSON columns, or more than one JSON column
**            An object is geneated from each input rows. Key names are generated from the corresponding metadata, values from the row. 
**
**  ARRAY:    A object is generated from row. The object contains a single key 'yadamuRow' and it's value is the array.
**
**  BSON:     ### A BOSN object is contructed based on the available metadata. 
**  
**
**  EXPORT: Implimented in dbParser.js
**          MongoDB support allows data from a MongoDB collection to be exported using one of the following mappings
**
**  DOCUMENT: MongoDB support allows the document to be exported as a single column of JSON. The _id column can be stripped from the generated documents
**
**  ARRAY :   A deep scan of the table is performed to derive the name and datatype of each key. 
**            Keys are mapped into an Array based on the results of the deep scan.
**
*/

class MongoDBI extends YadamuDBI {
	
  traceMongo(apiCall) {
	return `MongoClient.db(${this.connection.databaseName}).${apiCall}\n`
  }

  getMongoURL() {
    
    return `mongodb://${this.connectionProperties.host}:${this.connectionProperties.port}/${this.connectionProperties.database !== undefined ? this.connectionProperties.database : ''}`;
    
  }

  /*
  **
  ** Wrap all Mongo API calls used my YADAMU. Add tracing capability. No need generate meaningful stack traces, the ones generated by MongoError clases are acurate and complete
  **
  */

	
  async connect(options) {

    // Wrapper for client.db()
	
	const operation = `new MongoClient.connect ${this.getMongoURL()}))\n`
	try {	
	  if (this.status.sqlTrace) {
        this.status.sqlTrace.write(operation)
	  }
      let sqlStartTime = performance.now();
	  this.client = new MongoClient(this.getMongoURL(),options);
	  await this.client.connect();   
      this.traceTiming(sqlStartTime,performance.now())
	 } catch (e) {
	   throw new MongoError(e,operation)
	 }
  }

  async use(dbname,options) {

    // Wrapper for client.db(). db becomes the YADAMU connection. Needs to set this.connection for when it is used to change databases, and but also needs to return the connection for when it invoked by getConnectionFromPool()

	// this.yadamuLogger.trace([this.DATABASE_VENDOR,this.getWorkerNumber()],`mongoClient.db(${dbname})`)
    
	const operation = `client.db(${dbname})\n`
	try {	
	  if (this.status.sqlTrace) {
        this.status.sqlTrace.write(operation)
	  }
      let sqlStartTime = performance.now();
 	  options = options === undefined ? {returnNonCachedInstance:true} : (options.returnNonCachedInstance === undefined ? Object.assign (options, {returnNonCachedInstance:true} ) : options)
      this.connection = await this.client.db(dbname,options);	 
      this.traceTiming(sqlStartTime,performance.now())
      this.dbname = dbname;
      return this.connection
	} catch (e) {
	  throw new MongoError(e,operation)
	}
  }

  async command(command,options) {

    // Wrapper for db.command().

	// this.yadamuLogger.trace([this.DATABASE_VENDOR,this.getWorkerNumber()],`mongoClient.db(${dbname})`)
    
	const operation = `command(${JSON.stringify(command)})`
	try {	
	  if (this.status.sqlTrace) {
        this.status.sqlTrace.write(this.traceMongo(operation)) 
	  }
      let sqlStartTime = performance.now();
      const results = await this.connection.command(command,options);	 
      this.traceTiming(sqlStartTime,performance.now())
	  return results
	} catch (e) {
	  throw new MongoError(e,operation)
	}
  }

  async dbHash() {
	 const dbHash = {dbhash:1}
	 return await this.command(dbHash)
  }

  async dropDatabase(options) {

    // Wrapper for db.dropDatabase()

    
	const operation = `dropDatabase()`
	try {	
	  if (this.status.sqlTrace) {
        this.status.sqlTrace.write(this.traceMongo(operation)) 
	  }
      let sqlStartTime = performance.now();
	  
 	  await this.connection.dropDatabase() 
      this.traceTiming(sqlStartTime,performance.now())
	 } catch (e) {
	   throw new MongoError(e,operation)
	 }
  }

  async buildInfo() {
	  
	//  Wrapper for db.admin().buildInfo()  
	  
	
	const operation = `admin().buildInfo()`
	try {
      if (this.status.sqlTrace) {
        this.status.sqlTrace.write(this.traceMongo(operation))    
      }
	  const sqlStartTime = performance.now();
	  
      const results = await this.connection.admin().buildInfo()
      this.traceTiming(sqlStartTime,performance.now())
	  return results
    } catch (e) {
      throw new MongoError(e,operation);
	}
  }
  
  async stats(options) {
	  
    // Wrapper for db.stats()

	 
	 const operation = `stats()`
	 try {	
	   if (this.status.sqlTrace) {
         this.status.sqlTrace.write(this.traceMongo(operation)) 
	   }
       let sqlStartTime = performance.now();
	   
       this.stats = await this.connection.stats(options);	 
       this.traceTiming(sqlStartTime,performance.now())
	 } catch (e) {
	   throw new MongoError(e,operation)
	 }
  }

  async createCollection(collectionName,options) {
  
    // Wrapper for db.createCollection()

    const operation = `createCollection(${collectionName})`
    try {		
      if (this.status.sqlTrace) {
        this.status.sqlTrace.write(this.traceMongo(operation))    
      }      
      let sqlStartTime = performance.now();
      const collection = await this.connection.createCollection(collectionName);
      this.traceTiming(sqlStartTime,performance.now())
      return collection
	} catch (e) {
	  throw new MongoError(e,operation);
	}
  }

  async collection(collectionName,options) {

    // Wrapper for db.collection() - Note this return a Promise so has to be manually 'Promisfied'
  
    let stack	
    const operation = `collection(${collectionName})`
    try {		
      if (this.status.sqlTrace) {
        this.status.sqlTrace.write(this.traceMongo(operation))    
      }      
      let sqlStartTime = performance.now();
  	  const collection = await this.connection.collection(collectionName,options);
      this.traceTiming(sqlStartTime,performance.now())
      return collection
	} catch (e) {
	  throw new MongoError(e,operation);
	}
  }


  async collectionCount(collection,query,options) {

    // Wrapper for db.collection.count()
    
	
    const operation = `collection.count(${collection.namespace})`
	try {
      if (this.status.sqlTrace) {
        this.status.sqlTrace.write(this.traceMongo(operation))    
      }      
  	  let sqlStartTime = performance.now();
      
      const count = await collection.count(query,options)
      this.traceTiming(sqlStartTime,performance.now())
	  return count
	} catch (e) {
      throw new MongoError(e,operation);
	}
  }

  async collections(options)  {
  
    // Wrapper for db.collections()

    
	const operation = `collections()`
	try {
      if (this.status.sqlTrace) {
  	    this.status.sqlTrace.write(this.traceMongo(operation))    
      }
  	  let sqlStartTime = performance.now();
	  
      const collections = await this.connection.collections(options)
      this.traceTiming(sqlStartTime,performance.now())
      return collections;
	} catch (e) {
      throw new MongoError(e,operation);
	}
  }
  
  async insertOne(collectionName,doc,options) {

    // Wrapper for db.collection().insertOne()

	
	const operation = `collection(${collectionName}).insertOne()`
	try {
      if (this.status.sqlTrace) {
  	    this.status.sqlTrace.write(this.traceMongo(operation))    
      }
  	  let sqlStartTime = performance.now();
	  
	  options = options === undefined ? {w:1} : (options.w === undefined ? Object.assign (options, {w:1} ) : options)
      const results = await this.connection.collection(collectionName).insertOne(doc,{w:1});
      this.traceTiming(sqlStartTime,performance.now())
      return results;
	} catch (e) {
      throw new MongoError(e,operation);
	}
  }

  async insertMany(collectionName,array,options) {
	  
    // Wrapper for db.collection().insertMany()

	const operation = `collection(${collectionName}).insertMany(${array.length})`
	try {
      if (this.status.sqlTrace) {
  	    this.status.sqlTrace.write(this.traceMongo(operation))    
      }
  	  let sqlStartTime = performance.now();
	  options = options === undefined ? {w:1} : (options.w === undefined ? Object.assign (options, {w:1} ) : options)
      const results = await this.connection.collection(collectionName).insertMany(array,options);
      this.traceTiming(sqlStartTime,performance.now())
	  return results;
	} catch (e) {
      throw new MongoError(e,operation);
	}
  }


  validateIdentifiers(metadata) {
        
    // ### Todo Add better algorthim than simply stripping the invalid characters from the name
	// E.g. Check for Duplicates and use counter when duplicates are detected.
    
    const tableMappings = {}
    let mapTables = false;
    const tables = Object.keys(metadata)    
    tables.forEach((table,idx) => {
      const tableName = metadata[table].tableName
      if (tableName.indexOf('$') > -1) {
        mapTables = true;
        const newTableName = tableName.replace(/\$/g,'')
	    this.yadamuLogger.warning([this.DATABASE_VENDOR,tableName],`Mapped to "${newTableName}".`)
        tableMappings[table] = {tableName : newTableName}
        metadata[table].tableName = newTableName;
      }
    })        
	return mapTables ? tableMappings : undefined
  }
	
  async testConnection(connectionProperties) {   
    super.setConnectionProperties(connectionProperties);
	// ### Test Database connection
  }
	    
  async createConnectionPool() {

 	// this.yadamuLogger.trace([this.DATABASE_VENDOR,this.getWorkerNumber()],`createConnectionPool()`)
      
    this.logConnectionProperties();
	const poolSize = this.parameters.PARALLEL ? parseInt(this.parameters.PARALLEL) + 1 : 5
    this.connectionProperties.options = typeof this.connectionProperties.options === 'object' ? this.connectionProperties.options : {}
    if (poolSize > 5) {
	  this.connectionProperties.options.poolSize = poolSize
	}
	await this.connect(this.connectionProperties.options)
  }
  
  async getConnectionFromPool() {
	 return await this.use(this.dbname)
  }	
  
  async configureConnection() {
	  
 	// this.yadamuLogger.trace([this.DATABASE_VENDOR,this.getWorkerNumber()],`configureConnection()`)

    this.buildInformation = await this.buildInfo()
    this.dbVersion = this.buildInformation.version
  }
  
  async closeConnection() {
 	// this.db.close() ?
  }
  
  async closePool(options) {

	
	try {
	  let sqlStartTime = performance.now();
	  
      await this.client.close();   
      this.traceTiming(sqlStartTime,performance.now())
	} catch (e) {
      throw new MongoError(e,stack,'MongoClient()');
	}
  }

  async reconnectImpl() {
	// Default code for databases that support reconnection
    this.connection = this.isPrimary() ? await this.getConnectionFromPool() : await this.connectionProvider.getConnectionFromPool()

  }
  
  get DATABASE_VENDOR() { return 'MongoDB' };
  get SOFTWARE_VENDOR() { return 'MongoDB Corporation' };
  get SPATIAL_FORMAT()  { return 'GeoJSON' };
  get DEFAULT_PARAMETERS() { return this.yadamu.getYadamuDefaults().mongodb }
                                                               ;
  async executeDDLImpl(collectionList) {
    const results = await Promise.all(collectionList.map(async (collectionName) => {
	  await this.createCollection(collectionName)
    }));

  }    
    
  getConnectionProperties() {
  
    return{
      host             : this.parameters.HOSTNAME
     ,port             : this.parameters.PORT
    }
  }
  
  constructor(yadamu) {
    super(yadamu,yadamu.getYadamuDefaults().mongodb)
	
  }

  /*  
  **
  **  Connect to the database. Set global setttings
  **
  */
  
  async initialize() {   
	// TODO : Support for Mongo Authentication ???
    await super.initialize(false);   

	this.idTransformation = this.parameters.MONGO_STRIP_ID === true ? 'STRIP' : 'PRESERVE'

	switch (true) {
	  case ((this.parameters.MONGO_STORAGE_FORMAT === 'DOCUMENT') && (this.parameters.MONGO_EXPORT_FORMAT === 'ARRAY')) :
	    this.readTransformation = 'DOCUMENT_TO_ARRAY'
		break;
	  case ((this.parameters.MONGO_STORAGE_FORMAT === 'ARRAY') && (this.parameters.MONGO_EXPORT_FORMAT === 'DOCUMENT')) :
	    this.readTransformation  = 'ARRAY_TO_DOCUMENT'
		break;
      default:
	    this.readTransformation  = 'PRESERVE'
    } 
	
	this.writeTransformation = 'ARRAY_TO_DOCUMENT';
	
	this.yadamuLogger.info([`${this.DATABASE_VENDOR}`,`${this.dbVersion}`,`Configuration`],`Document ID Tranformation: ${this.idTransformation}.`)
	this.yadamuLogger.info([`${this.DATABASE_VENDOR}`,`${this.dbVersion}`,`Configuration`],`Read Tranformation: ${this.readTransformation}.`)
	this.yadamuLogger.info([`${this.DATABASE_VENDOR}`,`${this.dbVersion}`,`Configuration`],`Write Tranformation: ${this.writeTransformation}.`)
	
  }

  async finalize() {
	await super.finalize()
  } 

  /*
  **
  **  Abort the database connection and pool.
  **
  */

  async abort() {
									   
    await super.abort();
	  
  }

  /*
  **
  ** Begin the current transaction
  **
  */
  
  async initializeExport() {
	 super.initializeExport();
	 await this.use(this.parameters.FROM_USER);
  }
  
  async initializeImport() {
	 super.initializeImport();
	 await this.use(this.parameters.TO_USER);
  }
  
  /*
  **
  ** Begin a transaction
  **
  */
  
  async beginTransaction() {

     // this.yadamuLogger.trace([`${this.constructor.name}.beginTransaction()`,this.getWorkerNumber()],``)
	 super.beginTransaction();
							
  }

  // ### ToDO Support Mongo Transactions
  
  async commitTransaction() {
  }
	
  async rollbackTransaction(cause) {
  }

  async createSavePoint() {
  }
  
  async restoreSavePoint(cause) {
  }  

  async releaseSavePoint(cause) {
  } 

  /*
  **
  **  Upload a JSON File to the server. Optionally return a handle that can be used to process the file
  **
  */
  
  async uploadFile(importFilePath) {
  }
  
  /*
  **
  **  Process a JSON File that has been uploaded to the server. 
  **
  */

  async processFile(hndl) {
  }
  
  /*
  **
  ** The following methods are used by the YADAMU DBReader class
  **
  */
  
  /*
  **
  **  Generate the SystemInformation object for an Export operation
  **
  */
  
  async getSystemInformation() {     
 
   const stats = await this.stats();
   return {
     date               : new Date().toISOString()
    ,timeZoneOffset     : new Date().getTimezoneOffset()
    ,vendor             : this.DATABASE_VENDOR
    ,spatialFormat      : this.SPATIAL_FORMAT 
    ,schema             : this.parameters.FROM_USER
    ,softwareVendor     : this.SOFTWARE_VENDOR
    ,exportVersion      : this.EXPORT_VERSION
    ,nodeClient         : {
       version              : process.version
      ,architecture     : process.arch
      ,platform         : process.platform
     }
    ,buildInfo          : this.buildInformation
	,stats              : stats
  }
  }

  /*
  **
  **  Generate a set of DDL operations from the metadata generated by an Export operation
  **
  */

  async getDDLOperations() {
    return undefined
  }
  
  async getSchemaInfo(schema) {
	  
	const collections = await this.collections();
	const dbMetadata = await Promise.all(collections.map(async (collection) => {    
      const results = {TABLE_NAME: collection.collectionName, columns: ["JSON_DATA"], dataTypes: ["JSON"], sizeConstraints: [""]}
      if ((this.parameters.MONGO_STORAGE_FORMAT === 'DOCUMENT') && (this.parameters.MONGO_EXPORT_FORMAT === 'ARRAY')) {    	  
        let operation
     	try {
          operation = `db(${this.connection.databaseName}).collection(${collection.collectionName}.mapReduce()`;
          if (this.status.sqlTrace) {
            this.status.sqlTrace.write(this.traceMongo(operation))    
          }
    	  let sqlStartTime = performance.now();
		  const collectionMetadata = await collection.mapReduce(
		    // ### Do not try to use arrow functions or other ES2015+ features here...
		    function () { 
   		      // Emit 2 entries for this collection to force the reduce function to be invoked for this key..
		      if (Object.keys(keys).length === 0) {
   			    emit('metadata',null)
				emit('metadata',null)
			  }
			  Object.keys(this).forEach(function(key) { 
			    if (!keys.hasOwnProperty(key)) {
				  // Add an entry for this key to the set of known keys. There will be on entry for each key found in the collection.
                  // The value associated with the dataType key will be '' or the maximum length of the dataype based on the sample size.
				  // The value for this key will be an objet containing one key for each possible data type
				  keys[key] = {}
				}
				// Avoid the trap that the typeof returns object for a null value. 
				// We cannot determine anything about the possbile datatypes from a null value.
			    let dataType = this[key] === null ? null : typeof this[key]
				if (dataType === 'object') {
				  // Check for the special case of a mongo ObjectId object.
				  const objectType = Object.prototype.toString.call(this[key])
				  if (objectType === '[object ObjectId]') {
					dataType = 'ObjectId';
				  }
				}
                if ((dataType !== null) && (!keys[key].hasOwnProperty(dataType))) {
			      // If the datatype is not already known add an entry for this datatype with a value of ''
				  keys[key][dataType] = ''
				}
				if (dataType === 'string') {
				  // For strings make the length the lenghth of the largest string found.
				  dataTypeLength = this[key].length
				  const size = keys[key][dataType]
				  if (size === '' || (size < dataTypeLength)) {
					keys[key][dataType] = dataTypeLength
				  }
				}
				if (dataType === 'number') {
				  // For numbers capture the number of digits to the left and right of the decimal point
				  const components = this[key].toString().split(".")
				  const digitsLeft = components[0].length;
				  const digitsRight = components.length > 1 ? components[1].length : 0
				  const currentSizes = keys[key][dataType] === '' ? [0,0] : keys[key][dataType].split(",")
				  currentSizes[0] = digitsLeft > currentSizes[0] ? digitsLeft : currentSizes[0]
				  currentSizes[1] = digitsRight > currentSizes[1] ? digitsRight : currentSizes[1]
				  keys[key][dataType] = currentSizes.join(",")
				}				
		      },this)
			},
		    function(collectionName,values) {
			  // Uncomment to debug map reduce results
		      // return JSON.stringify(keys," ",2)
			  const keyNames = []
			  const dataTypes = []
			  const sizeConstraints = []
			  Object.keys(keys).forEach(function(key) {
				keyNames.push(key)
				const keyInfo = keys[key]
				const typeInfo = Object.keys(keyInfo)
				switch (typeInfo.length) {
				  case 0: 
				    // All values for the key were null..
				    dataTypes.push('string')
					sizeConstraints.push('16777216')
				    break;
				  case 1:
				    dataTypes.push(typeInfo[0])
      			    const maxSize = keys[key][typeInfo[0]]
					switch (typeInfo[0]) {
					  case 'string':
					    sizeConstraints.push(maxSize === '' ? '16777216' : maxSize)
					    break;
					  default:
					    sizeConstraints.push('')
					}
			   		break;
				  default:
				    dataTypes.push('JSON')
  				}
	          })
			  return {columns : keyNames, dataTypes : dataTypes, sizeConstraints: sizeConstraints}
			},
            {
              out       : { inline: 1}
			 ,limit     : this.parameters.MONGO_SCHEMA_ROWLIMIT === undefined ? 1000 : (this.parameters.MONGO_SCHEMA_ROWLIMIT === 0 ? null : this.parameters.MONGO_SCHEMA_ROWLIMIT)
             ,scope     : {
               keys     : {}
            }
          })
		  // Uncomnent to debug MapReduce results
		  // console.log(util.inspect(JSON.parse(collectionMetadata[0].value),{depth:null}))
          this.traceTiming(sqlStartTime,performance.now())
		  if (collectionMetadata.length === 1) {
			Object.assign(results,collectionMetadata[0].value)
		  }
        } catch(e) {
	      throw new MongoError(e,operation);
	    }
	  }
	  return results
    }))
	return dbMetadata
  }

  generateMetadata(dbMetadata,server) {    
    const metadata = {}
    for (let collection of dbMetadata) {
      if (this.idTransformation === 'STRIP') {
        const idx = collection.columns.indexOf('_id');
        if (idx > -1) {
          collection.columns.splice(idx,1);
          collection.dataTypes.splice(idx,1);
          collection.sizeConstraints.splice(idx,1);
        }
	  }
	  metadata[collection.TABLE_NAME] = {
       tableName                  : collection.TABLE_NAME
       ,columns                   : '"' + collection.columns.join('","') + '"'
       ,dataTypes                 : collection.dataTypes
       ,sizeConstraints           : collection.sizeConstraints
      }	  
    }
    return metadata;   
  }

  createParser(tableInfo,objectMode) {
	tableInfo.readTransformation = this.readTransformation
	tableInfo.idTransformation = this.idTransformation
	return new MongoParser(tableInfo,objectMode,this.yadamuLogger);
  }  
  
  streamingError(e,stack,collectionInfo) {
	const collectionName = collectionInfo.TABLE_NAME
    const operation = `db.collection(${collectionName}.find().stream()`
	return new MongoError(e,operation)
  }
  
  async getInputStream(collectionInfo,parser) {
	  
	const collectionName = collectionInfo.TABLE_NAME
    const readStream = new Readable({objectMode: true });
    readStream._read = () => {};

    
	const operation = `db.collection(${collectionName}.find().stream()`
	try {
      if (this.status.sqlTrace) {
        this.status.sqlTrace.write(operation)
      }
   	  let sqlStartTime = performance.now();
	  
      const mongoStream = await this.connection.collection(collectionName).find().stream();
      this.traceTiming(sqlStartTime,performance.now())
      mongoStream.on('data',(data) => {readStream.push(data)})
      mongoStream.on('end',(result) => {readStream.push(null)});
      mongoStream.on('error',(e) => {readStream.emit('error',e)});
      return readStream;      
    } catch (e) {
      throw new MongoError(e,operation)
	}
  }      
   
  /*
  **
  ** The following methods are used by the YADAMU DBwriter class
  **
  */
    
  async generateStatementCache(schema,executeDDL) {
    await super.generateStatementCache(StatementGenerator,schema,executeDDL) 
  }
  
  getOutputStream(primary) {
	 return super.getOutputStream(MongoWriter,primary)
  }
  
  
  async createSchema(schema) {  
    // ### Create a Database Schema or equivilant
	await this.use(schema)
  }

  async workerDBI(workerNumber) {
	const dbi = new MongoDBI(this.yadamu)
	dbi.readTransformation = this.readTransformation
	dbi.idTransformation = this.idTransformation
	return await super.workerDBI(workerNumber,dbi)
	dbi.db(this.stats.db);
  }

}

module.exports = MongoDBI
